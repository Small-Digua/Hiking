import pg from 'pg'
import dotenv from 'dotenv'
import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'

dotenv.config()

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

const dbUrl = process.env.DATABASE_URL

if (!dbUrl) {
  console.error('âŒ é”™è¯¯: æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚')
  process.exit(1)
}

const pool = new pg.Pool({
  connectionString: dbUrl,
  ssl: {
    rejectUnauthorized: false
  }
})

async function runHikingRecordColumnsMigration() {
  const client = await pool.connect()
  try {
    console.log('ğŸ”Œ æ­£åœ¨è¿æ¥æ•°æ®åº“...')
    
    // è¯»å–å¾’æ­¥è®°å½•å­—æ®µè¿ç§»æ–‡ä»¶
    const sqlPath = path.join(__dirname, '../supabase/migrations/20251228_add_hiking_record_columns.sql')
    const sql = fs.readFileSync(sqlPath, 'utf8')

    console.log('ğŸš€ å¼€å§‹æ‰§è¡Œå¾’æ­¥è®°å½•å­—æ®µè¿ç§»...')
    await client.query(sql)
    
    console.log('âœ… å¾’æ­¥è®°å½•å­—æ®µè¿ç§»æˆåŠŸï¼')
  } catch (err) {
    console.error('âŒ å¾’æ­¥è®°å½•å­—æ®µè¿ç§»å¤±è´¥:', err)
  } finally {
    client.release()
    await pool.end()
  }
}

runHikingRecordColumnsMigration()