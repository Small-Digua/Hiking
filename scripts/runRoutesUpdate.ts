import pg from 'pg'
import dotenv from 'dotenv'
import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'

dotenv.config()

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

const dbUrl = process.env.DATABASE_URL

if (!dbUrl) {
  console.error('âŒ é”™è¯¯: æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚')
  process.exit(1)
}

const pool = new pg.Pool({
  connectionString: dbUrl,
  ssl: {
    rejectUnauthorized: false
  }
})

async function runMigration() {
  const client = await pool.connect()
  try {
    console.log('ğŸ”Œ æ­£åœ¨è¿æ¥æ•°æ®åº“...')
    
    const sqlPath = path.join(__dirname, '../supabase/migrations/20251228_routes_update.sql')
    const sql = fs.readFileSync(sqlPath, 'utf8')

    console.log('ğŸš€ å¼€å§‹æ‰§è¡Œ Routes æ›´æ–°è„šæœ¬ (images, updated_at)...')
    await client.query(sql)
    
    console.log('âœ… Routes è¡¨æ›´æ–°æˆåŠŸï¼')
  } catch (err) {
    console.error('âŒ æ›´æ–°å¤±è´¥:', err)
  } finally {
    client.release()
    await pool.end()
  }
}

runMigration()
