import pg from 'pg'
import dotenv from 'dotenv'
import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'

dotenv.config()

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

const dbUrl = process.env.DATABASE_URL

if (!dbUrl) {
  console.error('âŒ é”™è¯¯: æœªæ‰¾åˆ° DATABASE_URL ç¯å¢ƒå˜é‡ã€‚')
  console.error('è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½® DATABASE_URL=postgresql://postgres:[PASSWORD]@db.xxxx.supabase.co:5432/postgres')
  process.exit(1)
}

const pool = new pg.Pool({
  connectionString: dbUrl,
  ssl: {
    rejectUnauthorized: false
  }
})

async function runMigrations() {
  const client = await pool.connect()
  try {
    console.log('ğŸ”Œ æ­£åœ¨è¿æ¥æ•°æ®åº“...')
    
    // è¯»å–æ‰€æœ‰è¿ç§»æ–‡ä»¶ï¼ŒæŒ‰æ–‡ä»¶åæ’åº
    const migrationsPath = path.join(__dirname, '../supabase/migrations')
    const migrationFiles = fs.readdirSync(migrationsPath)
      .filter(file => file.endsWith('.sql'))
      .sort()
    
    console.log(`ğŸ“‹ æ‰¾åˆ° ${migrationFiles.length} ä¸ªè¿ç§»æ–‡ä»¶:`)
    migrationFiles.forEach(file => console.log(`   - ${file}`))
    
    // è·³è¿‡åˆå§‹åŒ–è„šæœ¬ï¼Œåªæ‰§è¡Œåç»­çš„è¿ç§»è„šæœ¬
    // åˆå§‹åŒ–è„šæœ¬ä¼šåˆ é™¤æ‰€æœ‰æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»æ·»åŠ å­—æ®µçš„è¿ç§»å¼€å§‹
    const migrationsToRun = migrationFiles.filter(file => !file.includes('init_schema'))
    
    console.log(`\nğŸš€ å¼€å§‹æ‰§è¡Œ ${migrationsToRun.length} ä¸ªè¿ç§»è„šæœ¬...`)
    
    for (const file of migrationsToRun) {
      const sqlPath = path.join(migrationsPath, file)
      const sql = fs.readFileSync(sqlPath, 'utf8')
      
      console.log(`\nğŸ“„ æ‰§è¡Œè¿ç§»: ${file}`)
      try {
        await client.query(sql)
        console.log(`âœ… ${file} æ‰§è¡ŒæˆåŠŸï¼`)
      } catch (err) {
        console.error(`âŒ ${file} æ‰§è¡Œå¤±è´¥:`, err)
      }
    }
    
    console.log('\nğŸ‰ æ‰€æœ‰è¿ç§»è„šæœ¬æ‰§è¡Œå®Œæˆï¼')
  } catch (err) {
    console.error('âŒ è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯:', err)
  } finally {
    client.release()
    await pool.end()
  }
}

runMigrations()
